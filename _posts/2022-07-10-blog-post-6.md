---
title: 'How Text-Mining techniques are revolutionizing the precise treatment of cancer patients'
author: Vinod Singh
date: 2022-07-10
permalink: /posts/2022/07/blog-post-1/
  redirect_from: 
  - /md/
  - /markdown.html
tags:
  - Text Mining
  - Topic Modeling 
  - Mutation Signature Modeling
  - Cancer Genomics
---
  At many discussion platforms I have found that machine learning enthusiast are always interested to know how AI is helping in precise treatment of cancer patients. I always tell them many approaches which are dependent on the what data we have and in what question we are interested in?  But in this blog I am going to discuss a very interesting application of topic modeling approach which is getting widely popular in cancer patient specific clinical decision making.
  
  In the era of internet there are billions of documents are available online and presenting these documents to the users as collection of word makes no sense to them. So, it is always desirable to represent a documents as mixture of a few limited topics that can be understood by the user. To address this issue the topic modeling techniques of text mining are widely used to automate the document representation into limited number number of topics. Technically, the idea of topic modeling is to reduce the dimension of the documents from total unique word counts in the document database to limited number topics. Lets understand it with the help of an super easy hypothetical example, If we can represent each document in large corpus of documents which is having 4,000 unique words by 3 topics, then we reduced the dimension of a single document from 4,000 to 3. Which means our each document can be represented as point in a 3D orthogonal space, where each topic is representing a axis. The percentage of the 3 topics in a document will represent the exact location of the document in the 3D space. In this 3D space the documents which have similar distribution of topic percentage are clustered together and have many applications such as recommendation of similar document to the user or to the users of similar profile, organizing the documents etc. If you have noticed in this problem, we have not discussed yet, How to calculate the percentage of the topics in each document and How many distinct topics are suitable for our document corpus ? 
  
  
  
  
  $
    \begin{bmatrix}
           x_{1} \\
           x_{2} \\
           \vdots \\
           x_{m}
      \end{bmatrix}
 $ 
  
  Let me explain you the logic behind the calculations, the words co-occurring frequently in multiple documents are associated to a topic. The the co-occurrence frequency can be of `N-number` of words, but following precautions should be taken while choosing value of `N'.
  
 * if `N` is too low then we may end up with non-distinguishable topics.
 * if `N` is too high then we may may have majority of zero co-occurrence values (if document corpus size is not big enough) and our topics may be too unique i.e., they may not be seen again in any other document.
 
 The main crux from this is that if we consider high value of `N` to capture more contextual information we will need huge corpus of documents and computational infrastructure to do calculations on very high dimensional co-occurrence matrix of words. So, we have to do the trade off between our data size and value of `N`.  
 
These co-occurrence matrices have a lot of redundant and noisy features, that's why the can be decomposed into lower dimension matrices using dimensional reduction techniques such Non-negative matrix factorization (NMF) or Singular value decomposition (SVD).  
 
 The big tech giants like Google, Microsoft have access to billions of documents and huge computational infrastructure thats why their models for document recommendation performs very well.
 
 